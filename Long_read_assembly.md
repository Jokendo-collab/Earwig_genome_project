# Long read assembly pipeline
---
## 1. Basecalling (Guppy)
We used Guppy version 5.0.7 to basecall the `fast5` files produced by minions. It was ran in `GPU partition`.

`Script for Guppy`
```
#!/bin/bash -e

#SBATCH --job-name=Guppy_EW                 
#SBATCH --account=uoo02752              
#SBATCH --time=10:00:00                
#SBATCH --partition=gpu                 #guppy runs faster in gpu partition in Nesi, than other partition
#SBATCH --gres=gpu:1                    #some configuration for gpu partition, suggested by Nesi support
#SBATCH --mem=6G                                
#SBATCH --ntasks=4                              
#SBATCH --cpus-per-task=1               
#SBATCH --output=%x-%j.out              
#SBATCH --error=%x-%j.err               
#SBATCH --mail-type=ALL
#SBATCH --mail-user=bhaup057@student.otago.ac.nz

module load ont-guppy-gpu/5.0.7
guppy_basecaller -i ../path/to/all/fast5/files/ \
                 -s ./path/to/output/directory/ \
                 --flowcell FLO-MIN106 \
                 --kit SQK-LSK109 \
                 --num_callers 4 -x auto \
                 --recursive \                                  # recursive flag looks for fast5 files in subfolders as well
                 --trim_barcodes --disable_qscore_filtering     # disabled the quality filtering to get all the fastq files produced in one folder instead of pass, and fail. we will do QC later.
```
Then we concatenate all the fastq files produced in one fastq file `EW_nanopore_merged.fastq`


## 2. Quality check (PycoQC)
We used PycoQC to check the quality of our long read data. It produces an interactive html file with detailed description of data quality.
It uses `sequencing_summary.txt` file generated by guppy as an input.

`Script for PycoQC`
```
#!/bin/bash -e

#SBATCH --nodes 1
#SBATCH --cpus-per-task 1
#SBATCH --ntasks 5
#SBATCH --job-name PycoQc
#SBATCH --mem=2G
#SBATCH --time=00:20:00
#SBATCH --account=uoo02752
#SBATCH --output=%x.%j.out
#SBATCH --error=%x.%j.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=bhaup057@student.otago.ac.nz
#SBATCH --hint=nomultithread

export PATH="/nesi/nobackup/uoo02752/nematode/bin/miniconda3/bin:$PATH"

pycoQC -f path/to/sequencing_summary.txt            #sequencing_summary.txt is produced by guppy in its output folder.
        -o path/to/output/file/pycoQC_output.html
```
## 3. Filtering reads with Nanolyse
We have used `DNACS` during nanopore sequencing library preparation so we will use [Nanolyse](https://github.com/wdecoster/NanoLyse) to remove lamda DNA from our fastq files.

`Script for Nanolyse`
```
#!/bin/bash -e

#SBATCH --nodes 1
#SBATCH --cpus-per-task 1
#SBATCH --ntasks 5
#SBATCH --job-name Nanolyse.EW
#SBATCH --mem=1G
#SBATCH --time=01:00:00
#SBATCH --account=uoo02752
#SBATCH --output=%x.%j.out
#SBATCH --error=%x.%j.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=bhaup057@student.otago.ac.nz
#SBATCH --hint=nomultithread

module load NanoLyse/1.2.0-gimkl-2020a

cat path/to/EW_nanopore_merged.fastq | NanoLyse | gzip > EW_nanopore_merged_filtered.fastq.gz
```

